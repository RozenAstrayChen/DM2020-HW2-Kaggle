{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Information\n",
    "Name: 陳家麒\n",
    "\n",
    "Student ID: 109136501\n",
    "\n",
    "GitHub ID: RozenAstrayChen\n",
    "\n",
    "Kaggle name: Rozen Chen\n",
    "\n",
    "Team name: 我來抓人下去的\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "![image.png](attachment:784848b2-2625-4575-bc68-885e65605223.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2020-Lab2-Master Repo](https://github.com/fhcalderon87/DM2020-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2020-hw2-nthu/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the score (ie. 20% of 30% )\n",
    "\n",
    "    - **Top 41% - 100%**: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% )   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 5th 11:59 pm, Saturday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fhcalderon87/DM2020-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb), but make sure to fork the [DM2020-Lab2-Homework](https://github.com/fhcalderon87/DM2020-Lab2-Homework) repository this time! Also please __DON´T UPLOAD HUGE DOCUMENTS__, please use Git ignore for that.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 8th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collation\n",
    "\n",
    "This competition has two training data and submission data is hide in training data.\n",
    "So I plan:\n",
    "1. read data_identification.csv and read emotion.csv\n",
    "2. read tweets_DM.json and convert to dataframe\n",
    "3. combine data_identification, emotion and tweets_DM to one dataframe by __tweet_id__\n",
    "4. split training data and submission data by __identification__\n",
    "5. split training data and testing data by 7:3\n",
    "\n",
    "Now, we can use text to train our model. In detail you can see __collation.ipynb__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocess and model\n",
    "I think text is the most important about emotion, so I only use text to train.\n",
    "\n",
    "## preprocess+model+tuning\n",
    "In preprocess, I try use CountVectorizer, TfidfVectorizer, word2vector.\n",
    "In model, I try use Naive Bayesian, SVM, XGBClassifer, LogistcRegression, Bert.\n",
    "\n",
    "First I try bag2word + Naive Bayesian or other model but F1 score get 0.52, 0.48 in training and testing section, kaggle score just has 0.4.\n",
    "So I try use word2vector, but I have download __google_news__ pretrain model and transform training data. But I have no idea which will deadlock in training section\n",
    "maybe my computer is not good enough.\n",
    "\n",
    "Finally, I found one solution, I use TfidfVectorizer transform training data twice and combine together. First is tranform by text, second is word.\n",
    "This solution make my accuracy got 0.46 in kaggle and then I change my model logistcregression slover to 'sag' which can fit large data and speed up. This part make accuracy to 0.48.\n",
    "\n",
    "When I finished this experiment, time was running out. So I have fine-tuning in finially section. I found improve TfidfVectorizer max_feature can huge imprve accuracy, so I try improve to 100,000 and got accuracy 50!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import accuracy_score,matthews_corrcoef\n",
    "\n",
    "from tqdm import tqdm, trange,tnrange,tqdm_notebook\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "\n",
    "train_identification = pd.read_csv('dm2020-hw2-nthu/data_identification.csv')\n",
    "train_emotion = pd.read_csv('dm2020-hw2-nthu/emotion.csv')\n",
    "sample_submit = pd.read_csv('dm2020-hw2-nthu/sampleSubmission.csv')\n",
    "json_feature = pd.read_json('dm2020-hw2-nthu/tweets_DM.json', lines=True)\n",
    "data = pd.merge(train_identification, train_emotion, on=[\"tweet_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neaf/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/home/neaf/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# reference collation.ipynb.\n",
    "identification_train = data['identification'] == 'train'\n",
    "identification_test = train_identification['identification'] == 'test'\n",
    "\n",
    "train_X = data.loc[identification_train]\n",
    "test_X = train_identification.loc[identification_test]\n",
    "\n",
    "test_X['emotion'] = np.nan\n",
    "df = pd.concat([train_X, test_X])\n",
    "\n",
    "json_feature['hashtags'] = np.nan; json_feature['text'] = np.nan\n",
    "json_extend = pd.io.json.json_normalize(json_feature._source)\n",
    "json_extend['tweet.text'].isna().sum()\n",
    "json_feature['hashtags'] = json_extend['tweet.hashtags']\n",
    "json_feature['text'] = json_extend['tweet.text']\n",
    "json_feature['tweet_id'] = json_extend['tweet.tweet_id']\n",
    "\n",
    "df = pd.merge(df, json_feature, on=[\"tweet_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'trust', 'anticipation', 'sadness', 'disgust', 'fear',\n",
       "       'surprise', 'anger'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.loc[df['identification'] == 'train']\n",
    "df_test = df.loc[df['identification'] == 'test']\n",
    "\n",
    "df_train['emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neaf/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "df_train['emotion_enc'] = labelencoder.fit_transform(df_train['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = TfidfVectorizer(#min_df=150, \n",
    "                     max_features=1000000, \n",
    "                     strip_accents='unicode',\n",
    "                     analyzer='word',\n",
    "                     token_pattern=r'\\w{1,}',\n",
    "                     stop_words='english', \n",
    "                     ngram_range=(1,2),\n",
    ")\n",
    "\n",
    "cv2 = TfidfVectorizer(sublinear_tf=True,\n",
    "                      strip_accents='unicode',\n",
    "                      analyzer='char',\n",
    "                      stop_words='english',\n",
    "                      ngram_range=(2,6),\n",
    "                      max_features=1000000,\n",
    ")\n",
    "\n",
    "word = cv.fit_transform(df_train['text'])\n",
    "char = cv2.fit_transform(df_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack two feature\n",
    "from scipy.sparse import hstack\n",
    "text= hstack([word, char])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train, test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text,\n",
    "                                                    df_train['emotion'], \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifierCV\n",
    "\n",
    "mnb = LogisticRegression(solver='sag')\n",
    "mnb_tuning = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tuning.fit(X_train, y_train)\n",
    "predicted = mnb_tuning.predict(X_test)\n",
    "pred = mnb_tuning.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_train_score: 58.66%\n",
      "acc_train_score: 54.61%\n",
      "Precision: 74.01%\n",
      "Recall: 34.77%\n",
      "F1 Score:  0.38838487803609223\n",
      "[[  1058    493   1040     10   6574   2594      2    130]\n",
      " [     1  38700    733     27  31050   2938      8   1286]\n",
      " [     3   1099  11659     38  18065  10279      6    416]\n",
      " [     0   1214    547   3563  11676   1928      0    225]\n",
      " [     2   6991   1146     64 140032   3639     12   2946]\n",
      " [     1   1571   2933     50  25114  28108      9    504]\n",
      " [     0    540    816     14   9081   2730   1401    189]\n",
      " [     0   5157    740     12  39452   2091      6  13956]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nacc_train_score: 57.52%\\nacc_train_score: 57.53%\\nPrecision: 60.35%\\nRecall: 43.45%\\nupdate max\\nacc_train_score: 59.12%\\nacc_train_score: 57.86%\\nPrecision: 58.85%\\nRecall: 44.77%\\n\\n----iterator 10000----\\nacc_train_score: 56.07%\\nacc_train_score: 55.09%\\nPrecision: 53.01%\\nRecall: 40.25%\\n\\nlogist -> 25000\\nslove=sag\\nacc_train_score: 62.72%\\nacc_train_score: 59.40%\\n\\n-> 30000\\nacc_train_score: 63.07%\\nacc_train_score: 59.54%\\n\\n- 100000\\nbay\\nacc_train_score: 56.45%\\nacc_train_score: 53.93%\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use metric to check result\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "acc_test_score = metrics.accuracy_score(predicted,y_test)\n",
    "acc_train_score = metrics.accuracy_score(pred,y_train)\n",
    "\n",
    "prec_score = precision_score(y_test,predicted, average='macro')\n",
    "recall = recall_score(y_test, predicted,average='macro')\n",
    "f1 = f1_score(y_test,predicted,average='macro')\n",
    "matrix = confusion_matrix(y_test,predicted)\n",
    "\n",
    "print(str('acc_train_score: '+'{:04.2f}'.format(acc_train_score*100))+'%')\n",
    "print(str('acc_train_score: '+'{:04.2f}'.format(acc_test_score*100))+'%')\n",
    "\n",
    "print(str('Precision: '+'{:04.2f}'.format(prec_score*100))+'%')\n",
    "print(str('Recall: '+'{:04.2f}'.format(recall*100))+'%')\n",
    "print('F1 Score: ',f1)\n",
    "print(matrix)\n",
    "'''\n",
    "acc_train_score: 57.52%\n",
    "acc_train_score: 57.53%\n",
    "Precision: 60.35%\n",
    "Recall: 43.45%\n",
    "update max\n",
    "acc_train_score: 59.12%\n",
    "acc_train_score: 57.86%\n",
    "Precision: 58.85%\n",
    "Recall: 44.77%\n",
    "\n",
    "----iterator 10000----\n",
    "acc_train_score: 56.07%\n",
    "acc_train_score: 55.09%\n",
    "Precision: 53.01%\n",
    "Recall: 40.25%\n",
    "\n",
    "logist -> 25000\n",
    "slove=sag\n",
    "acc_train_score: 62.72%\n",
    "acc_train_score: 59.40%\n",
    "\n",
    "-> 30000\n",
    "acc_train_score: 63.07%\n",
    "acc_train_score: 59.54%\n",
    "\n",
    "- 100000\n",
    "bay\n",
    "acc_train_score: 56.45%\n",
    "acc_train_score: 53.93%\n",
    "- 1000000\n",
    "acc_train_score: 58.66%\n",
    "acc_train_score: 54.61%\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neaf/.local/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/neaf/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# train all data and predict submission data\n",
    "mnb.fit(text, df_train['emotion'])\n",
    "\n",
    "pred_word = cv.transform(df_test['text'])\n",
    "pred_char = cv2.transform(df_test['text'])\n",
    "pred_text = hstack([pred_word, pred_char])\n",
    "\n",
    "submit_pred = mnb.predict(pred_text)\n",
    "#result = labelencoder.inverse_transform(submit_pred)\n",
    "\n",
    "df_test['id'] = df_test['tweet_id']\n",
    "df_test['emotion'] = submit_pred\n",
    "\n",
    "df_test[['id', 'emotion']].to_csv('submission/TFIDF_logistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/neaf/.kaggle/kaggle.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.13M/6.13M [00:05<00:00, 1.11MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Successfully submitted to DM20 HW2 Kaggle Competition"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using kaggle api to push result\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "api.competition_submit('submission/TFIDF_logistic.csv', 'API Submission', 'dm2020-hw2-nthu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
